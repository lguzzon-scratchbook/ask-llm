#!/usr/bin/env node

const readline = require('node:readline');

const LLM_API_BASE_URL
	= process.env.LLM_API_BASE_URL || 'https://api.openai.com/v1';
const LLM_API_KEY = process.env.LLM_API_KEY || process.env.OPENAI_API_KEY;
const LLM_CHAT_MODEL = process.env.LLM_CHAT_MODEL;
const LLM_STREAMING = process.env.LLM_STREAMING !== 'no';

const LLM_DEBUG = process.env.LLM_DEBUG;

/**
 * Represents a chat message.
 *
 * @typedef {Object} Message
 * @property {'system'|'user'|'assistant'} role
 * @property {string} content
 */

/**
 * A callback function to stream then completion.
 *
 * @callback CompletionHandler
 * @param {string} text
 * @returns {void}
 */

/**
 * Generates a chat completion using a RESTful LLM API service.
 *
 * @param {Array<Message>} messages - List of chat messages.
 * @param {CompletionHandler=} handler - An optional callback to stream the completion.
 * @returns {Promise<string>} The completion generated by the LLM.
 */
const chat = async (messages, handler) => {
	const url = `${LLM_API_BASE_URL}/chat/completions`;
	const auth = LLM_API_KEY ? {Authorization: `Bearer ${LLM_API_KEY}`} : {};
	const model = LLM_CHAT_MODEL || 'gpt-4o-mini';
	const stop = ['<|im_end|>', '<|end|>', '<|eot_id|>'];
	const max_tokens = 200;
	const temperature = 0;
	const stream = LLM_STREAMING && typeof handler === 'function';
	try {
		const response = await fetch(url, {
			method: 'POST',
			headers: {'Content-Type': 'application/json', ...auth},
			body: JSON.stringify({
				messages,
				model,
				stop,
				max_tokens,
				temperature,
				stream,
			}),
		});
		if (!response.ok) {
			throw new Error(
				`HTTP error with the status: ${response.status} ${response.statusText}`,
			);
		}

		if (!stream) {
			const data = await response.json();
			const {choices} = data;
			const {message} = choices[0];
			const {content} = message;
			const answer = content.trim();
			handler?.(answer);
			return answer;
		}

		const reader = response.body.getReader();
		const decoder = new TextDecoder();
		let answer = '';
		let buffer = '';
		for (;;) {
			const {value, done} = await reader.read();
			if (done) {
				break;
			}

			const lines = decoder.decode(value).split('\n');
			for (const line of lines) {
				if (line.startsWith(':')) {
					continue;
				}

				if (line === 'data: [DONE]') {
					break;
				}

				if (line.length > 0) {
					const prefix = line.slice(0, 6);
					if (prefix === 'data: ') {
						const payload = line.slice(6);
						try {
							const jsonData = JSON.parse(payload);
							const {choices} = jsonData;
							const [choice] = choices;
							const {delta} = choice;
							const partial = delta?.content || '';
							if (partial && partial.length > 0) {
								answer += partial;
								handler?.(partial);
							}
						} catch {
							// ignore
						}
					} else {
						buffer += line;
						const [partial, rest] = buffer.split('data: ');
						buffer = rest || '';
						if (partial && partial.length > 0) {
							try {
								const jsonData = JSON.parse(partial);
								const {choices} = jsonData;
								const [choice] = choices;
								const {delta} = choice;
								const content = delta?.content || '';
								handler && content.length > 0 && handler(content);
								answer += content;
							} catch {
								// ignore
							}
						}
					}
				}
			}
		}

		return answer;
	} catch (error) {
		throw new Error(
			`Error occurred while making the request: ${error.message}`,
		);
	}
};

const SYSTEM_PROMPT = 'Answer the question politely and concisely.';

(async () => {
	console.log(`Using LLM at ${LLM_API_BASE_URL}.`);
	console.log('Press Ctrl+D to exit.');
	console.log();

	const messages = [];
	messages.push({role: 'system', content: SYSTEM_PROMPT});

	let loop = true;
	const io = readline.createInterface({
		input: process.stdin,
		output: process.stdout,
	});
	io.on('close', () => {
		loop = false;
	});

	const qa = () => {
		io.question('>> ', async question => {
			messages.push({role: 'user', content: question});
			const start = Date.now();
			const answer = await chat(messages, string_ =>
				process.stdout.write(string_),
			);
			messages.push({role: 'assistant', content: answer.trim()});
			console.log();
			const elapsed = Date.now() - start;
			LLM_DEBUG && console.log(`[${elapsed} ms]`);
			console.log();
			loop && qa();
		});
	};

	qa();
})();
